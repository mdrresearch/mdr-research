# RAIN: Towards Application-driven Benchmarking for Natural Language Processing

## M. Bartolo, J. Shangguan, K. Tylinski, W. Hernandez, E. Nemsic, B. Kultys, D. Hoadley, N. Roche,  A. P. Moore, G. Hinch, P. Stenetorp

### Abstract

With increasing availability of textual data and improved model capabilities, Natural Language Processing (NLP) is gaining wider adoption in industry.
However, the field is mainly guided by research-motivated benchmarks which, due to their research-oriented nature, 
can fail to adequately measure real-world utility of NLP applications.
We envision that, in addition to existing benchmarks, more application-driven benchmarks can also help guide us towards improved Natural Language Understanding.
To facilitate this, we introduce a definition of what we consider salient features for an applied benchmark and, as a first step in this direction, present the Real-World Applied Industrial NLP (RAIN) benchmark - a collection of NLP tasks and corresponding datasets with broad practical application.
We formulate five new NLP tasks, collect datasets for each totalling over 150,000 annotations, and provide evaluation of baseline and task-specific models, observing a headroom gap to human performance on the overall score of 19.4%.
The datasets and leaderboard are publicly available at https://rain.mdrdatascience.ai.

[paper](https://github.com/mdrresearch/mdr-research/blob/main/RAIN_Benchmark/RAIN_Towards_Application_driven_Benchmarking_for_Natural_Language_Processing.pdf)
