# Pre-trained Contextual Embeddings for Litigation Code Classification
## M. Bartolo, K. Tylinski, A. P. Moore

### Abstract

Models for a variety of natural language processing tasks, such as question answering or text classification, are potentially important components for a wide range of legal machine learning systems. These tasks may include examining whole legal corpora, but may also include a broad range of tasks that can support automation in the digital workplace. Importantly, recent advances in pre-trained contextual embeddings have substantially improved the performance of text classification across a wide range of tasks. In this paper, we investigate the application of these recent approaches on a legal time-recording task. We demonstrate improved performance on a 40-class J-code classification task over a variety of baseline techniques. The best performing single model achieves performance gains of 2.23 micro-averaged accuracy points and 9.39 macro-averaged accuracy points over the next best classifier on the test set. This result suggests these techniques will find broad utility in the development of legal language models for a range of automation tasks.

[web](https://www.mishcon.com/news/ai-for-legal-text-classification)

[paper](https://github.com/mdrresearch/mdr-research/blob/main/Pre-trained_Contextual_Embeddings/Pre-trained_Contextual_Embeddings.pdf)
